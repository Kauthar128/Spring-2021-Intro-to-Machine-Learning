{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Machine Learning, Data Matrix\n",
    "\n",
    "A machine learning problem generally has three parts\n",
    "\n",
    "* A task $T$\n",
    "* A performance measure $P$\n",
    "* An experience $E$\n",
    "\n",
    "A computer program learns from experience $E$ with respect to some task $T$ and performance measure $P$ if it improves at task $T$, measured by $P$, with experience $E$. For example,\n",
    "\n",
    "* **Task**: Identify pictures as having cats or not having cats in them\n",
    "* **Performance measure**: The proportion of pictures with cats does our computer properly identify\n",
    "* **Experience**: The computer will be given 500 images with cats and 500 images without cats, each with labels of \"cat\" or \"no cat\"\n",
    "\n",
    "Machine learning tasks are generally too difficult for pre-designed programs to do. Instead, we write programs for computers to learn to solve them. For example, we might want a machine learning algorithm to determine if a picture has a cat in it:\n",
    "\n",
    "* We **WOULD NOT** give a computer set of instructions to decide how to find cats. (What would the instructions be?!)\n",
    "\n",
    "* We **WOULD** feed many cat pictures and non-cat pictures to the computer, let label them according to some instructions (usually with some randomly initialized parameters), and give it some instructions for how to tweak its labels, and \n",
    "\n",
    "There are many types of tasks. Two large classes of tasks are supervised and unsupervised learning tasks.\n",
    "\n",
    "* **Supervised learning algorithms** have an experience of observing a dataset of examples *with* labels a correct algorithm would output.\n",
    "\n",
    "* **Unsupervised learning algorithms** have an experience of observing a dataset *without* labels and seek to learn useful patterns in the dataset.\n",
    "    \n",
    "There are some other types of problems, but these groups are most common and the primary focus of this class. Below, we discuss some common tasks in each category. It is not meant to be an exhaustive list but rather a brief outline of what the tasks are, examples of each, and some common methods for attacking these tasks.\n",
    "\n",
    "It should be noted that machine learning practitioners commonly use multiple methods, build methods customized to their problems, and create pipelines using multiple methods in a specified sequence.\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "For most supervised learning problems, we have a dataset of $n$ **examples** or **data points**, which can be presented as points in space\n",
    "\n",
    "$$ x_i=(x_{i1}, ..., x_{id})\\in\\mathbb{R}^d$$\n",
    "\n",
    "and we try to predict a function $f:\\mathbb{R}^d\\to\\mathbb{R}^m$ mapping these points $x_i$ to some **label** or **target** $y_i\\in\\mathbb{R}^m$. Each component of $x_i$ is usually called a **feature** or an **attribute**.\n",
    "\n",
    "### Classification Problems\n",
    "\n",
    "In a classification task, the learning algorithm tries to predict which of $k$ disjoint categories a datapoint belongs to--e.g. to estimate a function $f:\\mathbb{R}^d\\to\\{1, ..., k\\}$.\n",
    "\n",
    "Identifying pictures of cats is an example of this problem: the categories are \"cat\" and \"no cat,\" which are mutually exclusive. (Keep in mind an image is stored in a computer as a numerical value representing the intensity of red, blue, and green colors in each pixel of the image, which may easily be stored as a very high-dimensional point.)\n",
    "\n",
    "Common methods for classification include logistic regresion, $k$-nearest neighbors, the Bayes and naive Bayes classifiers, discriminant analysis (LDA and QDA), decision trees, random forests, and support vector machines--all of which will be covered in this course--as well as neural networks (MLPs, CNNs, RNNs), which are beyond the scope of this course.\n",
    "\n",
    "### Regression Problems\n",
    "\n",
    "In a regression task, the learning algorithm tries to predict a numerical $m$-vector given an $d$-dimensional input example--e.g. to estimate a function $f:\\mathbb{R}^d\\to\\mathbb{R}^m$.\n",
    "\n",
    "An example is predicting the price for which a house will sell given information on the house--the number of bedrooms, the number of bathrooms, the floorspace, the size of the surrounding yard, whether or not it has a pool, etc. Here, as in many of the problems we will cover, $m=1$, meaning we will predict only one output variable.\n",
    "\n",
    "Common methods for regression include linear regression, ridge and LASSO regression, decision trees, random forest, and support vector machines--all of which will be covered in this course--as well as linear models and neural networks (MLPs, CNNs, RNNs), which are beyond the scope of this course. \n",
    "\n",
    "## Unsupervised Learning\n",
    "\n",
    "In unsupervised learning, we do not have the benefit of knowing some of the results we need to find. It is usually a somewhat less structured search for useful patterns in a dataset.\n",
    "\n",
    "### Clustering Problems\n",
    "\n",
    "A clustering task is one that tries to find which datapoints are similar to one another, in some sense that we do not necessarily define in advance.\n",
    "\n",
    "Common methods for clustering are K-means and hierarchical clustering--both of which we will cover in the course--as well as mean-shift clustering, DBSCAN, Gaussian mixture models, and self-organizing maps, which are beyond the scope of this course.\n",
    "\n",
    "### Dimensionality Reduction\n",
    "\n",
    "In a dimensionality reduction task, the goal is to represent a dataset using fewer features without losing patterns in \n",
    "\n",
    "For example, if we have a 1-megapixel color picture as a datapoint, it would have 1 million pixels and we would store three numbers (R, G, and B values) for each pixel, meaning the dimension of the image would be 3 million. It is sometimes far too slow to use datapoints in $\\mathbb{R}^{3000000}$ in machine learning algorithms. It is frequently helpful to find ways to store datapoints in lower-dimensional spaces such that important information is not lost. The idea is similar to compression.\n",
    "\n",
    "Common methods for dimensionality reduction are discriminant analysis (LDA and QDA), and principal components analysis (PCA)--which we will cover in the course--as well as autoencoders and word-embeddings, which are beyond the scope of this course.\n",
    "\n",
    "### Anomaly Detection\n",
    "\n",
    "In an anomaly detection task, the goal is to find unusual patterns in data.\n",
    "\n",
    "An example is credit card companies trying to detect unusual usage of a credit card. If they can detect unusual activity, they sometimes deactive credit cards to avoid fraud. False positives may cause problems for legitimate customers, but cards deactivated due to actual fraud can prevent further damage. \n",
    "\n",
    "### Denoising\n",
    "\n",
    "In denoising, the goal is to uncover some original dataset that has been corrupted by some sort of noise, whether we mean noisy sounds or random error. In all cases, the goal is to find a sometimes-faint signal within some noise.\n",
    "\n",
    "Denoising is not a task we will cover explicitly in the course.\n",
    "\n",
    "Examples:\n",
    "\n",
    "* If you are speaking into a phone while riding in a car, the road noise can cause the voice signal not to be transmitted clearly\n",
    "* Noise-cancelling headphones try to counteract noise to pass through clear audio\n",
    "* Random errors in measurements in particle physics make the signal difficult to extract\n",
    "* Financial instruments like stocks can fluctuate at random while there is an underlying cause of general trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Matrix\n",
    "\n",
    "In this section, we will cover how data is frequently stored such that it can be used by machine learning methods. It should not be thought that this is the only way or always the best way to store data, but it will be a series of conventions used in many fields.\n",
    "\n",
    "For many problems, we will have a **dataset** consisting of some number $n$ points in $\\mathbb{R}^d$ that we will store in a matrix\n",
    "\n",
    "$$\n",
    "X = \\begin{pmatrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1d}\\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2d}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x_{n1} & x_{n2} & \\cdots & x_{nd}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The $i$th *row* $x_i = (x_{i1}, x_{i2}, ..., x_{id})\\in\\mathbb{R}^d$ is the $i$th point in the dataset. These points have many names in different fields: **points, datapoints, examples, vectors, records, feature-vectors**. In some sources, these points $x_i$ are denoted $\\mathbf{x}_i$, but it should be clear that $x$ with a single subscripts indicates a point while $x$ with two subscripts is the component of a point.\n",
    "\n",
    "The $j$th *column* $X_j=(x_{1j}, x_{2j}, ..., x_{nj})\\in\\mathbb{R}^n$ is the $j$th component of each point in the dataset. These are likewise called by many names dependent on field: **features, attributes, variables, dimensions, properties, fields**. In some cases, each column can be considered a random sample of a random variable, or the rows of the matrix $X$ can be considered a random sample of vector-valued random variables.\n",
    "\n",
    "The number of points $n$ is the **size** of the dataset and the length of the points $d$ is called the **dimensionality** of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
